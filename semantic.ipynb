{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a9c8a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.4.0-cp312-cp312-win_amd64.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (1.3.2)\n",
      "Requirement already satisfied: rouge-score in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (0.1.2)\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.9.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from gensim) (1.26.3)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from gensim) (1.11.4)\n",
      "Collecting smart_open>=1.8.1 (from gensim)\n",
      "  Downloading smart_open-7.5.0-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn) (3.2.0)\n",
      "Requirement already satisfied: absl-py in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from rouge-score) (2.2.0)\n",
      "Requirement already satisfied: six>=1.14.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from rouge-score) (1.16.0)\n",
      "Requirement already satisfied: click in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from nltk) (8.1.7)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2025.11.3-cp312-cp312-win_amd64.whl.metadata (41 kB)\n",
      "Collecting tqdm (from nltk)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting wrapt (from smart_open>=1.8.1->gensim)\n",
      "  Downloading wrapt-2.0.1-cp312-cp312-win_amd64.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading gensim-4.4.0-cp312-cp312-win_amd64.whl (24.4 MB)\n",
      "   ---------------------------------------- 0.0/24.4 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 5.2/24.4 MB 31.9 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 14.2/24.4 MB 37.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.4/24.4 MB 41.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 24.4/24.4 MB 37.7 MB/s eta 0:00:00\n",
      "Downloading nltk-3.9.2-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.5/1.5 MB 40.4 MB/s eta 0:00:00\n",
      "Downloading regex-2025.11.3-cp312-cp312-win_amd64.whl (277 kB)\n",
      "Downloading smart_open-7.5.0-py3-none-any.whl (63 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading wrapt-2.0.1-cp312-cp312-win_amd64.whl (60 kB)\n",
      "Installing collected packages: wrapt, tqdm, regex, smart_open, nltk, gensim\n",
      "Successfully installed gensim-4.4.0 nltk-3.9.2 regex-2025.11.3 smart_open-7.5.0 tqdm-4.67.1 wrapt-2.0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "chromadb 0.6.3 requires tenacity>=8.2.3, which is not installed.\n",
      "chromadb 0.6.3 requires typer>=0.9.0, which is not installed.\n",
      "datasets 4.3.0 requires dill<0.4.1,>=0.3.0, which is not installed.\n",
      "datasets 4.3.0 requires fsspec[http]<=2025.9.0,>=2023.1.0, which is not installed.\n",
      "huggingface-hub 0.36.0 requires fsspec>=2023.5.0, which is not installed.\n",
      "openai-whisper 20250625 requires more-itertools, which is not installed.\n",
      "openai-whisper 20250625 requires numba, which is not installed.\n",
      "opentelemetry-api 1.30.0 requires importlib-metadata<=8.5.0,>=6.0, which is not installed.\n",
      "ragas 0.3.8 requires appdirs, which is not installed.\n",
      "ragas 0.3.8 requires networkx, which is not installed.\n",
      "ragas 0.3.8 requires typer, which is not installed.\n",
      "tensorflow-intel 2.16.1 requires h5py>=3.10.0, which is not installed.\n",
      "deprecated 1.2.18 requires wrapt<2,>=1.10, but you have wrapt 2.0.1 which is incompatible.\n",
      "opentelemetry-instrumentation 0.51b0 requires wrapt<2.0.0,>=1.0.0, but you have wrapt 2.0.1 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim scikit-learn rouge-score nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506dbbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt', quiet=True)  # Tokenization\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # Suppress Gensim warnings\n",
    "\n",
    "def load_text(file_path):\n",
    "    \"\"\"Load text from file.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            return f.read().strip()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File {file_path} not found. Create it with sample text.\")\n",
    "        return \"\"\n",
    "\n",
    "def get_word2vec_embedding(text, model, unknown_vector=np.zeros(300)):\n",
    "    \"\"\"Compute averaged Word2Vec embedding for a text.\"\"\"\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    valid_embeddings = []\n",
    "    for token in tokens:\n",
    "        if token in model:\n",
    "            valid_embeddings.append(model[token])\n",
    "        else:\n",
    "            valid_embeddings.append(unknown_vector)  # Fallback for OOV\n",
    "    if valid_embeddings:\n",
    "        return np.mean(valid_embeddings, axis=0)\n",
    "    return unknown_vector\n",
    "\n",
    "def semantic_similarity_gensim(human_text, generated_text, model_name='word2vec-google-news-300'):\n",
    "    \"\"\"Compute semantic similarity using averaged Word2Vec embeddings.\"\"\"\n",
    "    # Load pre-trained Word2Vec (downloads ~1.5GB first time)\n",
    "    try:\n",
    "        model = api.load(model_name)\n",
    "    except Exception:\n",
    "        print(f\"Error loading {model_name}. Ensure Gensim is installed and try again.\")\n",
    "        return 0.0\n",
    "    \n",
    "    human_emb = get_word2vec_embedding(human_text, model)\n",
    "    gen_emb = get_word2vec_embedding(generated_text, model)\n",
    "    \n",
    "    # Cosine similarity (reshape for sklearn)\n",
    "    sim_matrix = cosine_similarity(human_emb.reshape(1, -1), gen_emb.reshape(1, -1))\n",
    "    sim = sim_matrix[0][0]\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7aaa5b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Texts (first 100 chars):\n",
      "Human: So let's start with the feedback. All right, let's go through it. So I would probably estimate your ...\n",
      "Generated: Part 1：\n",
      "\n",
      "1. **Overall Band Score**: 7.0 – The candidate speaks at length with generally good fluency...\n",
      "\n",
      "==================================================\n",
      "Semantic Similarity (Word2Vec Cosine): 0.8127\n",
      "\n",
      "Lexical Alignment:\n",
      "BLEU Score: 0.0000\n",
      "ROUGE-L F1: 0.1058\n",
      "\n",
      "Overall Alignment (threshold=0.7): Aligned (score=0.8127)\n",
      "\n",
      "Insight: High semantic overlap—texts share contextual word meanings.\n"
     ]
    }
   ],
   "source": [
    "human_file = 'human.txt'\n",
    "generated_file = 'generated.txt'\n",
    "    \n",
    "human_text = load_text(human_file)\n",
    "generated_text = load_text(generated_file)\n",
    "    \n",
    "if not human_text or not generated_text:\n",
    "    print(\"Error: Please create 'human.txt' and 'generated.txt' with your texts.\")\n",
    "    exit(1)\n",
    "    \n",
    "print(\"Loaded Texts (first 100 chars):\")\n",
    "print(f\"Human: {human_text[:100]}...\")\n",
    "print(f\"Generated: {generated_text[:100]}...\")\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "    \n",
    "# Gensim Word2Vec similarity (semantic via word vectors)\n",
    "sem_sim = semantic_similarity_gensim(human_text, generated_text)\n",
    "print(f\"Semantic Similarity (Word2Vec Cosine): {sem_sim:.4f}\")\n",
    "    \n",
    "if sem_sim > 0.8:\n",
    "    print(\"\\nInsight: High semantic overlap—texts share contextual word meanings.\")\n",
    "elif sem_sim > 0.5:\n",
    "    print(\"\\nInsight: Moderate alignment—related vocabulary, but topics diverge slightly.\")\n",
    "else:\n",
    "    print(\"\\nInsight: Low similarity—distinct semantic fields; potential content mismatch.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
